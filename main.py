import io
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import pdfplumber
from bs4 import BeautifulSoup
import re
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

IS_LOCAL = os.getenv('ENVIRONMENT') != 'production'

if IS_LOCAL:
    try:
        from sentence_transformers import SentenceTransformer, util
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é SBERT –º–æ–¥–µ–ª—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞...")
        SBERT_MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("‚úÖ SBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    except ImportError:
        print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF")
        SBERT_MODEL = None
else:
    SBERT_MODEL = None
    print("‚òÅÔ∏è Render —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—ë–≥–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã")

app = FastAPI(title="Quiz Helper API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

SESSION_STORAGE = {}

class QuizHtmlRequest(BaseModel):
    html: str = Field(..., min_length=1)

class ProcessQuizRequest(BaseModel):
    questions: List[Dict[str, Any]]
    lecture_text: str = Field(..., min_length=1)

# === –£–¢–ò–õ–ò–¢–´ ===

def normalize_text(s):
    if not s:
        return ""
    return re.sub(r'\s+', ' ', s).strip().lower()

def extract_full_sentences(text, position, num_sentences=2):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    if not text or position < 0 or position >= len(text):
        return ""
    
    sentence_start = 0
    for i in range(position - 1, -1, -1):
        if text[i] in '.!?\n':
            sentence_start = i + 1
            break
    
    while sentence_start < len(text) and text[sentence_start] in ' \n\r\t':
        sentence_start += 1
    
    sentence_end = position
    sentences_found = 0
    for i in range(position, len(text)):
        if text[i] in '.!?':
            sentences_found += 1
            if sentences_found >= num_sentences:
                sentence_end = i + 1
                break
    
    if sentences_found < num_sentences:
        sentence_end = min(len(text), position + 400)
    
    result = text[sentence_start:sentence_end].strip()
    result = re.sub(r'\s+', ' ', result)
    return result

def calculate_text_similarity_tfidf(text1, text2):
    try:
        vectorizer = TfidfVectorizer(min_df=1, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        words1 = set(normalize_text(text1).split())
        words2 = set(normalize_text(text2).split())
        if not words1 or not words2:
            return 0.0
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0

def calculate_text_similarity_sbert(text1, text2):
    if SBERT_MODEL is None:
        return calculate_text_similarity_tfidf(text1, text2)
    
    try:
        embeddings = SBERT_MODEL.encode([text1, text2], convert_to_tensor=True)
        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
        return float(similarity)
    except:
        return calculate_text_similarity_tfidf(text1, text2)

def calculate_text_similarity(text1, text2):
    if IS_LOCAL and SBERT_MODEL is not None:
        return calculate_text_similarity_sbert(text1, text2)
    else:
        return calculate_text_similarity_tfidf(text1, text2)

def find_definition_for_question(lecture, question_text):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ—Ä–º–∏–Ω –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
    question_normalized = normalize_text(question_text)
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: —É–±–∏—Ä–∞–µ–º "—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ" –∏–∑ –ö–û–ù–¶–ê –≤–æ–ø—Ä–æ—Å–∞
    # –ü–∞—Ç—Ç–µ—Ä–Ω: "... ‚Äì —ç—Ç–æ <input/> —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ."
    question_normalized = re.sub(r'\s*—ç—Ç–æ\s+\S*\s*—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ\s*\.?\s*$', '', question_normalized)
    question_normalized = re.sub(r'\s+—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ\s*\.?\s*$', '', question_normalized)
    
    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    for phrase in ['–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ', '—ç—Ç–æ –æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å']:
        question_normalized = question_normalized.replace(phrase, '')
    
    question_normalized = question_normalized.strip()
    
    stop_words = {'—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–æ–±–æ–π', 
                  '–∏–ª–∏', '–¥–ª—è', '–ø—Ä–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–Ω–∏—Ö', '–æ–Ω–∞', '–æ–Ω–æ'}
    question_keywords = [w for w in question_normalized.split() 
                        if len(w) > 3 and w not in stop_words]
    
    if len(question_keywords) < 3:
        return None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω: –¢–ï–†–ú–ò–ù - —ç—Ç–æ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï
    pattern = r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,60})\s*(?:[‚Äî\-:]|[\s]+-[\s]+)\s*—ç—Ç–æ\s+([^.!?]{20,400}[.!?])'
    
    best_match = None
    best_score = 0.0
    
    for match in re.finditer(pattern, lecture, re.IGNORECASE):
        full_term = match.group(1).strip()
        definition = match.group(2).strip()
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ—Ä–º–∏–Ω–µ (–µ—Å–ª–∏ —Å–ª–æ–≤–∞ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –ø–æ–¥—Ä—è–¥)
        term_parts = full_term.split()
        cleaned_parts = []
        for i, part in enumerate(term_parts):
            if i == 0 or part.lower() != term_parts[i-1].lower():
                cleaned_parts.append(part)
        
        term = ' '.join(cleaned_parts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
        similarity = calculate_text_similarity(definition, question_text)
        
        # –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        definition_normalized = normalize_text(definition)
        keyword_matches = sum(1 for kw in question_keywords if kw in definition_normalized)
        keyword_ratio = keyword_matches / len(question_keywords) if question_keywords else 0
        
        combined_score = (similarity * 0.5) + (keyword_ratio * 0.5)
        
        if combined_score > best_score and combined_score > 0.5:
            best_score = combined_score
            best_match = {
                "term": term,
                "definition": definition,
                "position": match.start(),
                "score": combined_score
            }
    
    return best_match

def score_option_by_lecture(lecture, option, question=""):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–ø—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–µ–∫—Ü–∏–∏"""
    L = normalize_text(lecture)
    opt = normalize_text(option)
    q = normalize_text(question)
    
    score = 0
    snippets = []
    
    # –°–¢–†–û–ì–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è –î–û–ó
    is_dose_units_question = False
    if '–µ–¥–∏–Ω–∏—Ü' in q and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in q and '–¥–æ–∑' in q:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω" –ò–õ–ò "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω"
        has_equiv = '—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω' in q
        has_eff = '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω' in q
        if has_equiv or has_eff:
            is_dose_units_question = True
    
    # –¢–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
    exact_pattern = re.escape(opt)
    exact_matches = list(re.finditer(exact_pattern, L))
    exact_count = len(exact_matches)
    
    if exact_count > 0:
        base_score = 2.5 * (1 + exact_count)**0.4
        
        best_snippet = None
        has_correct_dose_context = False
        
        for match in exact_matches:
            match_pos = match.start()
            
            # –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            context_start = max(0, match_pos - 300)
            context_end = min(len(L), match_pos + 300)
            context = L[context_start:context_end]
            
            # –î–ª—è –≤–æ–ø—Ä–æ—Å–∞ –ø—Ä–æ –¥–æ–∑—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º –°–¢–†–û–ì–û–ï –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö —Å–ª–æ–≤
            if is_dose_units_question:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º "—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω" –∏–ª–∏ "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω" + "–∏–∑–º–µ—Ä–µ–Ω–∏—è"
                has_equiv_meas = ('—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω' in context and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in context) or '—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–π –¥–æ–∑—ã' in context
                has_eff_meas = ('—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω' in context and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in context) or '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–æ–∑—ã' in context
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ù–ï "–ø–æ–≥–ª–æ—â–µ–Ω–Ω–∞—è –¥–æ–∑–∞"
                is_absorbed = '–ø–æ–≥–ª–æ—â–µ–Ω–Ω' in context
                
                if (has_equiv_meas or has_eff_meas) and not is_absorbed:
                    has_correct_dose_context = True
                    orig_pos = lecture.lower().find(opt, match_pos - 10)
                    if orig_pos != -1:
                        best_snippet = extract_full_sentences(lecture, orig_pos, 2)
                    break
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–æ–Ω—É—Å/—à—Ç—Ä–∞—Ñ
        if is_dose_units_question:
            if has_correct_dose_context:
                # –ú–ï–ì–ê –ë–û–ù–£–° –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü
                base_score *= 15.0
                if best_snippet:
                    snippets.append({
                        "why": "exact_dose_context",
                        "excerpt": best_snippet
                    })
            else:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –®–¢–†–ê–§ –¥–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö
                base_score *= 0.005
        else:
            # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞
            if best_snippet is None:
                orig_pos = lecture.lower().find(opt)
                if orig_pos != -1:
                    best_snippet = extract_full_sentences(lecture, orig_pos, 2)
            
            if best_snippet:
                snippets.append({
                    "why": "exact",
                    "excerpt": best_snippet
                })
        
        score += base_score
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    def_patterns = [
        rf"{re.escape(opt)}\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]+[.!?])",
        rf"{re.escape(opt)}\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]+[.!?])",
    ]
    
    for pat in def_patterns:
        for match in re.finditer(pat, lecture, re.IGNORECASE):
            definition = match.group(1) if len(match.groups()) > 0 else ""
            
            def_normalized = normalize_text(definition)
            q_words = [w for w in q.split() if len(w) > 3]
            match_count = sum(1 for w in q_words if w in def_normalized)
            
            bonus = 4.0
            if match_count > 0:
                bonus *= (1 + match_count * 0.3)
            
            score += bonus
            full_sentence = extract_full_sentences(lecture, match.start(), 2)
            snippets.append({
                "why": f"definition",
                "excerpt": full_sentence
            })
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ (–ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –≤ snippets)
    opt_words = set(opt.split())
    if opt_words and len(opt_words) > 1:
        matched_words = len(opt_words.intersection(set(L.split())))
        ratio = matched_words / len(opt_words)
        score += ratio * 1.5
    
    return {"score": score, "snippets": snippets}

def detect_question_type(qtext):
    q = normalize_text(qtext)
    
    if re.search(r'(–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|—Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|–≤–ø–∏—à–∏—Ç–µ|–≤–≤–µ–¥–∏—Ç–µ)', qtext, re.IGNORECASE):
        return 'short'
    
    if '–µ–¥–∏–Ω–∏—Ü' in q and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in q:
        return 'units'
    
    single_markers = ['–∫–∞–∫–æ–µ –∏–∑', '–∫–∞–∫–æ–π –∏–∑', '–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '—á—Ç–æ –∏–∑', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫–æ–µ .* –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç']
    for marker in single_markers:
        if re.search(marker, q):
            return 'single'
    
    multi_markers = ['–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤—Ö–æ–¥—è—Ç –≤', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è', '–Ω–∞–∑–æ–≤–∏—Ç–µ –≤—Å–µ', 
                     '–∫–∞–∫–æ–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è']
    for marker in multi_markers:
        if marker in q:
            return 'multi'
    
    return 'single'

def parse_html_quiz(html):
    soup = BeautifulSoup(html, 'html.parser')
    questions = []
    
    que_elements = soup.find_all(class_='que')
    
    for el in que_elements:
        q = {}
        qtext_el = el.find(class_='qtext')
        if qtext_el:
            for tag in qtext_el.find_all(['label', 'input']):
                tag.decompose()
            q['question'] = qtext_el.get_text(strip=True).replace('\n', ' ')
        else:
            q['question'] = f"–í–æ–ø—Ä–æ—Å {len(questions) + 1}"
        
        opts = []
        answer_divs = el.find_all(class_='answer')
        for div in answer_divs:
            labels = div.find_all(attrs={'data-region': 'answer-label'})
            for label in labels:
                opt_text = label.get_text(strip=True).replace('\n', ' ')
                if opt_text:
                    opts.append(opt_text)
            for label in div.find_all('label'):
                if not label.find_parent(class_='qtext'):
                    opt_text = label.get_text(strip=True).replace('\n', ' ')
                    if opt_text and opt_text not in opts:
                        opts.append(opt_text)
        
        q['options'] = list(set(opts))
        q['is_short'] = bool(el.find('input', type='text')) or 'shortanswer' in el.get('class', [])
        questions.append(q)
    
    if not questions:
        all_ps = soup.find_all('p')
        all_inputs = soup.find_all('input', type='radio') + soup.find_all('input', type='checkbox')
        for p in all_ps:
            if p.find('input'):
                continue
            question_text = p.get_text(strip=True).replace('\n', ' ')
            if question_text:
                options = []
                for inp in all_inputs:
                    label = soup.find('label', attrs={'for': inp.get('id')})
                    if label:
                        opt_text = label.get_text(strip=True).replace('\n', ' ')
                        if opt_text:
                            options.append(opt_text)
                questions.append({
                    "question": question_text,
                    "options": list(set(options)),
                    "is_short": bool(soup.find('input', type='text'))
                })
                break
    
    return questions

# === –ú–ê–†–®–†–£–¢–´ ===

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    mode = "üöÄ –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (SBERT)" if (IS_LOCAL and SBERT_MODEL) else "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π —Ä–µ–∂–∏–º (TF-IDF)"
    return templates.TemplateResponse("index.html", {"request": request, "mode": mode})

@app.post("/api/extract-text-from-pdf/")
async def extract_text_from_pdf(file: UploadFile = File(...)):
    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 10MB)")
    
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
    
    session_key = "default"
    SESSION_STORAGE[session_key] = text
    
    return {
        "text": text,
        "length": len(text),
        "snippet": text[:200]
    }

@app.post("/api/parse-quiz-html/")
async def parse_quiz_html(data: QuizHtmlRequest):
    html = data.html
    if not html:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'html' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    try:
        questions = parse_html_quiz(html)
    except Exception as e:
        print(f"Error in parse_html_quiz: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML —Ç–µ—Å—Ç–∞: {str(e)}")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–í HTML –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.")
    
    return {"ok": True, "questions": questions}

@app.post("/api/process-quiz/")
async def process_quiz(data: ProcessQuizRequest):
    questions = data.questions
    lecture_text = data.lecture_text
    
    if not lecture_text:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'lecture_text' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'questions' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    results = []
    
    for q in questions:
        qtext = q.get("question", "")
        qtype = detect_question_type(qtext)
        opts = q.get("options", [])
        is_short = q.get("is_short", False)
        
        if is_short or qtype == 'short':
            match = find_definition_for_question(lecture_text, qtext)
            
            if match:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": match["term"],
                    "excerpt": extract_full_sentences(lecture_text, match["position"], 2),
                })
            else:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": "",
                    "excerpt": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                })
            continue
        
        scored = []
        for opt in opts:
            score_result = score_option_by_lecture(lecture_text, opt, qtext)
            scored.append({
                "option": opt,
                "score": score_result["score"],
                "snippets": score_result["snippets"]
            })
        
        max_score = max([s["score"] for s in scored], default=1)
        for s in scored:
            s["norm"] = round(s["score"] / max_score, 3) if max_score > 0 else 0
        
        selected = []
        
        if qtype == 'single':
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                top = sorted_scores[0]
                selected = [{
                    "option": top["option"],
                    "score": top["norm"],
                    "snippets": top["snippets"]
                }]
        
        elif qtype == 'units':
            # –ü—Ä–æ—Å—Ç–æ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-1 –ø–æ score (–ª–æ–≥–∏–∫–∞ —É–∂–µ –≤ scoring)
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                selected = [{
                    "option": sorted_scores[0]["option"],
                    "score": sorted_scores[0]["norm"],
                    "snippets": sorted_scores[0]["snippets"]
                }]
        
        else:  # multi
            candidates = [s for s in scored if s["norm"] >= 0.5]
            if not candidates:
                candidates = [s for s in scored if s["norm"] >= 0.3]
            selected = [
                {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                for s in candidates
            ]
            selected.sort(key=lambda x: x["score"], reverse=True)
        
        results.append({
            "question": qtext,
            "type": qtype,
            "options": [{"option": s["option"], "norm": s["norm"], "snippets": s["snippets"]} for s in scored],
            "selected": selected
        })
    
    return {"ok": True, "results": results}