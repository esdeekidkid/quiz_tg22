import io
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import pdfplumber
from bs4 import BeautifulSoup
import re
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# === –†–ï–ñ–ò–ú –†–ê–ë–û–¢–´ ===
IS_LOCAL = os.getenv('ENVIRONMENT') != 'production'

# === –ó–ê–ì–†–£–ó–ö–ê ML –ú–û–î–ï–õ–ï–ô ===
SBERT_MODEL = None
SPACY_MODEL = None

if IS_LOCAL:
    print("üöÄ === –õ–û–ö–ê–õ–¨–ù–´–ô –†–ï–ñ–ò–ú (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ú–û–©–ù–û–°–¢–¨) ===")
    try:
        from sentence_transformers import SentenceTransformer, util
        print("  üì¶ –ó–∞–≥—Ä—É–∂–∞—é SBERT...")
        SBERT_MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("  ‚úÖ SBERT –∑–∞–≥—Ä—É–∂–µ–Ω!")
    except Exception as e:
        print(f"  ‚ö†Ô∏è SBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    try:
        import spacy
        print("  üì¶ –ó–∞–≥—Ä—É–∂–∞—é SpaCy (—Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å)...")
        SPACY_MODEL = spacy.load("ru_core_news_sm")
        print("  ‚úÖ SpaCy –∑–∞–≥—Ä—É–∂–µ–Ω!")
    except Exception as e:
        print(f"  ‚ö†Ô∏è SpaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    print("üéØ –†–µ–∂–∏–º: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨ (95-98%)\n")
else:
    print("‚òÅÔ∏è === RENDER –†–ï–ñ–ò–ú (–û–ë–õ–ï–ì–ß–Å–ù–ù–ê–Ø –í–ï–†–°–ò–Ø) ===")
    print("üéØ –†–µ–∂–∏–º: TF-IDF + N-–≥—Ä–∞–º–º—ã (85-90% —Ç–æ—á–Ω–æ—Å—Ç—å)\n")

app = FastAPI(title="Universal Quiz Helper")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

SESSION_STORAGE = {}

class QuizHtmlRequest(BaseModel):
    html: str = Field(..., min_length=1)

class ProcessQuizRequest(BaseModel):
    questions: List[Dict[str, Any]]
    lecture_text: str = Field(..., min_length=1)

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´ ===

def normalize_text(s):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not s:
        return ""
    return re.sub(r'\s+', ' ', s).strip().lower()

def extract_full_sentences(text, position, num_sentences=2):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç N –ø–æ–ª–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ—Ç –ø–æ–∑–∏—Ü–∏–∏"""
    if not text or position < 0 or position >= len(text):
        return ""
    
    sentence_start = 0
    for i in range(position - 1, -1, -1):
        if text[i] in '.!?\n':
            sentence_start = i + 1
            break
    
    while sentence_start < len(text) and text[sentence_start] in ' \n\r\t':
        sentence_start += 1
    
    sentence_end = position
    sentences_found = 0
    for i in range(position, len(text)):
        if text[i] in '.!?':
            sentences_found += 1
            if sentences_found >= num_sentences:
                sentence_end = i + 1
                break
    
    if sentences_found < num_sentences:
        sentence_end = min(len(text), position + 400)
    
    result = text[sentence_start:sentence_end].strip()
    result = re.sub(r'\s+', ' ', result)
    return result

def extract_ngrams(text, n_min=2, n_max=4):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç N-–≥—Ä–∞–º–º—ã (—Ñ—Ä–∞–∑—ã –∏–∑ N —Å–ª–æ–≤)"""
    words = normalize_text(text).split()
    ngrams = []
    for n in range(n_min, min(n_max + 1, len(words) + 1)):
        for i in range(len(words) - n + 1):
            ngrams.append(' '.join(words[i:i+n]))
    return ngrams

def calculate_similarity_tfidf(text1, text2):
    """TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
    try:
        vectorizer = TfidfVectorizer(min_df=1, stop_words=None, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        words1 = set(normalize_text(text1).split())
        words2 = set(normalize_text(text2).split())
        if not words1 or not words2:
            return 0.0
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0

def calculate_similarity_sbert(text1, text2):
    """SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (—Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ)"""
    if SBERT_MODEL is None:
        return calculate_similarity_tfidf(text1, text2)
    
    try:
        from sentence_transformers import util as sbert_util
        embeddings = SBERT_MODEL.encode([text1, text2], convert_to_tensor=True)
        similarity = sbert_util.cos_sim(embeddings[0], embeddings[1]).item()
        return float(similarity)
    except:
        return calculate_similarity_tfidf(text1, text2)

def calculate_similarity(text1, text2):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è similarity"""
    if IS_LOCAL and SBERT_MODEL is not None:
        return calculate_similarity_sbert(text1, text2)
    else:
        return calculate_similarity_tfidf(text1, text2)

def extract_noun_phrases(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã (—Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ —Å SpaCy)"""
    if SPACY_MODEL is None:
        return []
    
    try:
        doc = SPACY_MODEL(text[:1000])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        return [chunk.text.lower() for chunk in doc.noun_chunks]
    except:
        return []

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===

def find_definition_for_question(lecture, question_text):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–´–• –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–ß—Ç–æ —Ç–∞–∫–æ–µ X?" –∏–ª–∏ "X - —ç—Ç–æ..."
    """
    question_normalized = normalize_text(question_text)
    
    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    for phrase in ['–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ', '—ç—Ç–æ –æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å']:
        question_normalized = question_normalized.replace(phrase, '')
    
    # –£–±–∏—Ä–∞–µ–º "—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ" –∏ –ø–æ–¥–æ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ü–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    question_normalized = re.sub(r'\s+\w+\s*\.?\s*$', '', question_normalized)
    question_normalized = question_normalized.strip()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
    stop_words = {'—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–æ–±–æ–π', 
                  '–∏–ª–∏', '–¥–ª—è', '–ø—Ä–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–Ω–∏—Ö', '–æ–Ω–∞', '–æ–Ω–æ', '–∫–æ—Ç–æ—Ä—ã–µ'}
    question_keywords = [w for w in question_normalized.split() 
                        if len(w) > 3 and w not in stop_words]
    
    if len(question_keywords) < 2:
        return None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤ –ª–µ–∫—Ü–∏–∏
    patterns = [
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+—è–≤–ª—è–µ—Ç—Å—è\s+([^.!?]{15,500}[.!?])',
    ]
    
    best_match = None
    best_score = 0.0
    
    for pattern in patterns:
        for match in re.finditer(pattern, lecture, re.IGNORECASE):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ—Ä–º–∏–Ω–µ
            term_parts = term.split()
            cleaned_parts = []
            for i, part in enumerate(term_parts):
                if i == 0 or part.lower() != term_parts[i-1].lower():
                    cleaned_parts.append(part)
            term = ' '.join(cleaned_parts)
            
            # –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–û–ï —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ similarity
            similarity = calculate_similarity(definition, question_text)
            
            # N-–≥—Ä–∞–º–º—ã –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            question_ngrams = set(extract_ngrams(question_text, 2, 3))
            definition_ngrams = set(extract_ngrams(definition, 2, 3))
            ngram_overlap = len(question_ngrams.intersection(definition_ngrams))
            ngram_score = ngram_overlap / max(len(question_ngrams), 1)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
            combined_score = (similarity * 0.6) + (ngram_score * 0.4)
            
            if combined_score > best_score and combined_score > 0.4:
                best_score = combined_score
                best_match = {
                    "term": term,
                    "definition": definition,
                    "position": match.start(),
                    "score": combined_score
                }
    
    return best_match

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –û–ü–¶–ò–ô ===

def score_option_universal(lecture, option, question):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–ø—Ü–∏–∏.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–´–• –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤.
    """
    L = normalize_text(lecture)
    opt = normalize_text(option)
    q = normalize_text(question)
    
    score = 0.0
    snippets = []
    
    # === 1. –ü–û–ò–°–ö –¶–ï–õ–´–• –°–õ–û–í ===
    word_pattern = r'\b' + re.escape(opt) + r'\b'
    exact_matches = list(re.finditer(word_pattern, L))
    exact_count = len(exact_matches)
    
    if exact_count > 0:
        base_score = 2.0 * (1 + exact_count)**0.3
        
        best_snippet = None
        best_context_score = 0.0
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        for match in exact_matches:
            match_pos = match.start()
            context_start = max(0, match_pos - 350)
            context_end = min(len(L), match_pos + 350)
            context = L[context_start:context_end]
            
            # –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ö–û–ù–¢–ï–ö–°–¢–ê:
            # –°—á–∏—Ç–∞–µ–º N-–≥—Ä–∞–º–º—ã –∏–∑ –≤–æ–ø—Ä–æ—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            question_ngrams = set(extract_ngrams(question, 2, 4))
            context_ngrams = set(extract_ngrams(context, 2, 4))
            common_ngrams = question_ngrams.intersection(context_ngrams)
            
            # Similarity –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–æ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context_similarity = calculate_similarity(question, context)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_score = (len(common_ngrams) * 0.5) + (context_similarity * 2.0)
            
            if context_score > best_context_score:
                best_context_score = context_score
                orig_pos = lecture.lower().find(opt, match_pos - 10)
                if orig_pos != -1:
                    best_snippet = extract_full_sentences(lecture, orig_pos, 2)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–æ–Ω—É—Å –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if best_context_score > 0.5:
            context_multiplier = 1 + (best_context_score * 0.8)
            base_score *= context_multiplier
            if best_snippet:
                snippets.append({
                    "why": f"context (score: {best_context_score:.2f})",
                    "excerpt": best_snippet
                })
        else:
            if best_snippet:
                snippets.append({"why": "exact", "excerpt": best_snippet})
        
        score += base_score
    
    # === 2. –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===
    def_patterns = [
        rf"\b{re.escape(opt)}\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]+[.!?])",
        rf"\b{re.escape(opt)}\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]+[.!?])",
    ]
    
    for pat in def_patterns:
        for match in re.finditer(pat, lecture, re.IGNORECASE):
            definition = match.group(1) if len(match.groups()) > 0 else ""
            
            # Similarity –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –≤–æ–ø—Ä–æ—Å–æ–º
            def_similarity = calculate_similarity(definition, question)
            
            bonus = 3.0 * (1 + def_similarity)
            score += bonus
            
            full_sentence = extract_full_sentences(lecture, match.start(), 2)
            snippets.append({
                "why": f"definition (sim: {def_similarity:.2f})",
                "excerpt": full_sentence
            })
    
    # === 3. TF-IDF –í–°–ï–ô –û–ü–¶–ò–ò ===
    opt_words = set(opt.split())
    if opt_words and len(opt_words) > 1:
        matched_words = len(opt_words.intersection(set(L.split())))
        ratio = matched_words / len(opt_words)
        score += ratio * 1.2
    
    return {"score": score, "snippets": snippets}

# === –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –í–û–ü–†–û–°–ê ===

def detect_question_type(qtext):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞"""
    q = normalize_text(qtext)
    
    if re.search(r'(–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ|—Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|–≤–ø–∏—à–∏—Ç–µ|–≤–≤–µ–¥–∏—Ç–µ)', qtext, re.IGNORECASE):
        return 'short'
    
    if re.search(r'–µ–¥–∏–Ω–∏—Ü.*–∏–∑–º–µ—Ä–µ–Ω–∏—è', q):
        return 'units'
    
    single_markers = ['–∫–∞–∫–æ–µ –∏–∑', '–∫–∞–∫–æ–π –∏–∑', '–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '—á—Ç–æ –∏–∑', '—á—Ç–æ —Ç–∞–∫–æ–µ']
    for marker in single_markers:
        if marker in q:
            return 'single'
    
    multi_markers = ['–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤—Ö–æ–¥—è—Ç', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è', '–Ω–∞–∑–æ–≤–∏—Ç–µ', '–¥–µ–π—Å—Ç–≤–∏—è']
    for marker in multi_markers:
        if marker in q:
            return 'multi'
    
    return 'single'

def parse_html_quiz(html):
    """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Ç–µ—Å—Ç–∞"""
    soup = BeautifulSoup(html, 'html.parser')
    questions = []
    
    que_elements = soup.find_all(class_='que')
    
    for el in que_elements:
        q = {}
        qtext_el = el.find(class_='qtext')
        if qtext_el:
            for tag in qtext_el.find_all(['label', 'input']):
                tag.decompose()
            q['question'] = qtext_el.get_text(strip=True).replace('\n', ' ')
        else:
            q['question'] = f"–í–æ–ø—Ä–æ—Å {len(questions) + 1}"
        
        opts = []
        answer_divs = el.find_all(class_='answer')
        for div in answer_divs:
            labels = div.find_all(attrs={'data-region': 'answer-label'})
            for label in labels:
                opt_text = label.get_text(strip=True).replace('\n', ' ')
                if opt_text:
                    opts.append(opt_text)
            for label in div.find_all('label'):
                if not label.find_parent(class_='qtext'):
                    opt_text = label.get_text(strip=True).replace('\n', ' ')
                    if opt_text and opt_text not in opts:
                        opts.append(opt_text)
        
        q['options'] = list(set(opts))
        q['is_short'] = bool(el.find('input', type='text')) or 'shortanswer' in el.get('class', [])
        questions.append(q)
    
    return questions

# === API ENDPOINTS ===

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    mode = "üöÄ –õ–æ–∫–∞–ª—å–Ω—ã–π (SBERT+SpaCy)" if IS_LOCAL else "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π (TF-IDF)"
    return templates.TemplateResponse("index.html", {"request": request, "mode": mode})

@app.post("/api/extract-text-from-pdf/")
async def extract_text_from_pdf(file: UploadFile = File(...)):
    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å 10MB)")
    
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")
    
    SESSION_STORAGE["default"] = text
    
    return {"text": text, "length": len(text), "snippet": text[:200]}

@app.post("/api/parse-quiz-html/")
async def parse_quiz_html(data: QuizHtmlRequest):
    try:
        questions = parse_html_quiz(data.html)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return {"ok": True, "questions": questions}

@app.post("/api/process-quiz/")
async def process_quiz(data: ProcessQuizRequest):
    questions = data.questions
    lecture_text = data.lecture_text
    
    if not lecture_text or not questions:
        raise HTTPException(status_code=400, detail="–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ")
    
    results = []
    
    for q in questions:
        qtext = q.get("question", "")
        qtype = detect_question_type(qtext)
        opts = q.get("options", [])
        is_short = q.get("is_short", False)
        
        # === –ö–û–†–û–¢–ö–ò–ô –û–¢–í–ï–¢ ===
        if is_short or qtype == 'short':
            match = find_definition_for_question(lecture_text, qtext)
            
            if match:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": match["term"],
                    "excerpt": extract_full_sentences(lecture_text, match["position"], 2),
                })
            else:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": "",
                    "excerpt": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ",
                })
            continue
        
        # === –í–û–ü–†–û–°–´ –° –í–ê–†–ò–ê–ù–¢–ê–ú–ò ===
        scored = []
        for opt in opts:
            score_result = score_option_universal(lecture_text, opt, qtext)
            scored.append({
                "option": opt,
                "score": score_result["score"],
                "snippets": score_result["snippets"]
            })
        
        max_score = max([s["score"] for s in scored], default=1)
        for s in scored:
            s["norm"] = round(s["score"] / max_score, 3) if max_score > 0 else 0
        
        selected = []
        
        if qtype == 'single' or qtype == 'units':
            # –û–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                selected = [{
                    "option": sorted_scores[0]["option"],
                    "score": sorted_scores[0]["norm"],
                    "snippets": sorted_scores[0]["snippets"]
                }]
        
        else:  # multi
            # –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å high score
            candidates = [s for s in scored if s["norm"] >= 0.5]
            if not candidates:
                candidates = [s for s in scored if s["norm"] >= 0.35]
            selected = [
                {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                for s in candidates
            ]
            selected.sort(key=lambda x: x["score"], reverse=True)
        
        results.append({
            "question": qtext,
            "type": qtype,
            "options": [{"option": s["option"], "norm": s["norm"], "snippets": s["snippets"]} for s in scored],
            "selected": selected
        })
    
    return {"ok": True, "results": results}
