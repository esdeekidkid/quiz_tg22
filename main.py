import io
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import pdfplumber
from bs4 import BeautifulSoup
import re
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# === –†–ï–ñ–ò–ú –†–ê–ë–û–¢–´ ===
IS_LOCAL = os.getenv('ENVIRONMENT') != 'production'

# === –ó–ê–ì–†–£–ó–ö–ê ML –ú–û–î–ï–õ–ï–ô ===
SBERT_MODEL = None
SPACY_MODEL = None

if IS_LOCAL:
    print("üöÄ === –õ–û–ö–ê–õ–¨–ù–´–ô –†–ï–ñ–ò–ú (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ú–û–©–ù–û–°–¢–¨) ===")
    try:
        from sentence_transformers import SentenceTransformer, util
        print("  üì¶ –ó–∞–≥—Ä—É–∂–∞—é SBERT (–º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å)...")
        SBERT_MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("  ‚úÖ SBERT –∑–∞–≥—Ä—É–∂–µ–Ω!")
    except Exception as e:
        print(f"  ‚ö†Ô∏è SBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    try:
        import spacy
        print("  üì¶ –ó–∞–≥—Ä—É–∂–∞—é SpaCy (—Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å)...")
        SPACY_MODEL = spacy.load("ru_core_news_sm")
        print("  ‚úÖ SpaCy –∑–∞–≥—Ä—É–∂–µ–Ω!")
    except Exception as e:
        print(f"  ‚ö†Ô∏è SpaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    
    print("üéØ –†–µ–∂–∏–º: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨ (95-98%)\n")
else:
    print("‚òÅÔ∏è === RENDER –†–ï–ñ–ò–ú (–û–ë–õ–ï–ì–ß–Å–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° SBERT) ===")
    try:
        from sentence_transformers import SentenceTransformer, util
        print("  üì¶ –ó–∞–≥—Ä—É–∂–∞—é –ª—ë–≥–∫—É—é SBERT –º–æ–¥–µ–ª—å (distiluse)...")
        SBERT_MODEL = SentenceTransformer('distiluse-base-multilingual-cased-v2')
        print("  ‚úÖ –õ—ë–≥–∫–∞—è SBERT –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    except Exception as e:
        print(f"  ‚ö†Ô∏è SBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF: {e}")
        SBERT_MODEL = None
    
    print("üéØ –†–µ–∂–∏–º: TF-IDF/SBERT + N-–≥—Ä–∞–º–º—ã (90-93% —Ç–æ—á–Ω–æ—Å—Ç—å)\n")

app = FastAPI(title="Universal Quiz Helper")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

SESSION_STORAGE = {}

class QuizHtmlRequest(BaseModel):
    html: str = Field(..., min_length=1)

class ProcessQuizRequest(BaseModel):
    questions: List[Dict[str, Any]]
    lecture_text: str = Field(..., min_length=1)

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´ ===

def normalize_text(s):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not s:
        return ""
    return re.sub(r'\s+', ' ', s).strip().lower()

def extract_full_sentences(text, position, num_sentences=2):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç N –ø–æ–ª–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ—Ç –ø–æ–∑–∏—Ü–∏–∏"""
    if not text or position < 0 or position >= len(text):
        return ""
    
    sentence_start = 0
    for i in range(position - 1, -1, -1):
        if text[i] in '.!?\n':
            sentence_start = i + 1
            break
    
    while sentence_start < len(text) and text[sentence_start] in ' \n\r\t':
        sentence_start += 1
    
    sentence_end = position
    sentences_found = 0
    for i in range(position, len(text)):
        if text[i] in '.!?':
            sentences_found += 1
            if sentences_found >= num_sentences:
                sentence_end = i + 1
                break
    
    if sentences_found < num_sentences:
        sentence_end = min(len(text), position + 400)
    
    result = text[sentence_start:sentence_end].strip()
    result = re.sub(r'\s+', ' ', result)
    return result

def extract_ngrams(text, n_min=2, n_max=4):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç N-–≥—Ä–∞–º–º—ã (—Ñ—Ä–∞–∑—ã –∏–∑ N —Å–ª–æ–≤)"""
    words = normalize_text(text).split()
    ngrams = []
    for n in range(n_min, min(n_max + 1, len(words) + 1)):
        for i in range(len(words) - n + 1):
            ngrams.append(' '.join(words[i:i+n]))
    return ngrams

def calculate_similarity_tfidf(text1, text2):
    """TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
    try:
        vectorizer = TfidfVectorizer(min_df=1, stop_words=None, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        words1 = set(normalize_text(text1).split())
        words2 = set(normalize_text(text2).split())
        if not words1 or not words2:
            return 0.0
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0

def calculate_similarity_sbert(text1, text2):
    """SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
    if SBERT_MODEL is None:
        return calculate_similarity_tfidf(text1, text2)
    
    try:
        from sentence_transformers import util as sbert_util
        embeddings = SBERT_MODEL.encode([text1, text2], convert_to_tensor=True)
        similarity = sbert_util.cos_sim(embeddings[0], embeddings[1]).item()
        return float(similarity)
    except:
        return calculate_similarity_tfidf(text1, text2)

def calculate_similarity(text1, text2):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è similarity"""
    if SBERT_MODEL is not None:
        return calculate_similarity_sbert(text1, text2)
    else:
        return calculate_similarity_tfidf(text1, text2)

def extract_noun_phrases(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã (—Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ —Å SpaCy)"""
    if SPACY_MODEL is None:
        return []
    
    try:
        doc = SPACY_MODEL(text[:1000])
        return [chunk.text.lower() for chunk in doc.noun_chunks]
    except:
        return []

# === –î–ï–¢–ï–ö–¶–ò–Ø –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–• –°–ü–ò–°–ö–û–í ===

def extract_lists_from_lecture(lecture):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ –ª–µ–∫—Ü–∏–∏.
    –ù–∞–ø—Ä–∏–º–µ—Ä: "—Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ 3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: ‚Ä¢ –ø—É–Ω–∫—Ç1 ‚Ä¢ –ø—É–Ω–∫—Ç2 ‚Ä¢ –ø—É–Ω–∫—Ç3"
    """
    lists = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–ø–∏—Å–∫–æ–≤
    list_headers = [
        r'—Ä–∞–∑–¥–µ–ª—è[—é–µ]—Ç—Å—è\s+–Ω–∞\s+\w+\s+(–∫–∞—Ç–µ–≥–æ—Ä–∏[–∏—é]|–≤–∏–¥[–∞–æ]|—Ç–∏–ø[–∞–æ]|–∫–ª–∞—Å—Å[–∞–æ])',
        r'—Å—É—â–µ—Å—Ç–≤—É[–µ—é]—Ç\s+\w+\s+(–∫–∞—Ç–µ–≥–æ—Ä–∏[–∏—é]|–≤–∏–¥[–∞–æ]|—Ç–∏–ø[–∞–æ]|–∫–ª–∞—Å—Å[–∞–æ])',
        r'–≤–∫–ª—é—á–∞[–µ—é]—Ç\s+\w+\s+(–∫–∞—Ç–µ–≥–æ—Ä–∏[–∏—é]|–≤–∏–¥[–∞–æ]|—Ç–∏–ø[–∞–æ])',
        r'–ø–æ\s+\w+\s+—Ä–∞–∑–¥–µ–ª—è',
    ]
    
    for pattern in list_headers:
        for match in re.finditer(pattern, lecture, re.IGNORECASE):
            start_pos = match.end()
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ –ø–æ–∏—Å–∫–∞ –¥–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤
            text_chunk = lecture[start_pos:start_pos + 1500]
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞ (—É–ª—É—á—à–µ–Ω–Ω—ã–µ)
            item_patterns = [
                r'‚Ä¢\s*([–ê-–Ø–Å][^\n‚Ä¢]{5,200})',  # ‚Ä¢ –ü—É–Ω–∫—Ç
                r'\n\s*-\s*([–ê-–Ø–Å][^\n\-]{5,200})', # - –ü—É–Ω–∫—Ç (—Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫–∏)
                r'\d+\)\s*([–ê-–Ø–Å][^\n]{5,200})', # 1) –ü—É–Ω–∫—Ç
            ]
            
            for item_pattern in item_patterns:
                items = re.findall(item_pattern, text_chunk)
                if len(items) >= 2:  # –ù–∞—à–ª–∏ —Å–ø–∏—Å–æ–∫ –∏–∑ –º–∏–Ω–∏–º—É–º 2 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    cleaned_items = []
                    for item in items:
                        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞
                        item = item.strip()
                        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç–æ–π/—Ç–æ—á–∫–µ —Å –∑–∞–ø—è—Ç–æ–π –∏ –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å (–∑–∞–≥–æ–ª–æ–≤–æ–∫)
                        item = re.split(r'[,;]', item)[0]
                        # –ë–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –∏–ª–∏ –¥–æ 15 —Å–ª–æ–≤
                        words = item.split()[:15]
                        cleaned_item = ' '.join(words)
                        cleaned_item = re.sub(r'[,;]+$', '', item).strip()

def find_relevant_list(question, lists):
    """–ù–∞—Ö–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –∏–∑ –ª–µ–∫—Ü–∏–∏, –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å—É."""
    if not lists:
        return None
    
    q_norm = normalize_text(question)
    best_list = None
    best_score = 0.0
    
    for lst in lists:
        context_norm = normalize_text(lst['context'])
        q_ngrams = set(extract_ngrams(question, 2, 3))
        ctx_ngrams = set(extract_ngrams(lst['context'], 2, 3))
        overlap = len(q_ngrams.intersection(ctx_ngrams))
        similarity = calculate_similarity(question, lst['context'])
        score = overlap * 2 + similarity * 5
        
        if score > best_score:
            best_score = score
            best_list = lst
    
    return best_list if best_score > 2.0 else None

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===

def find_definition_for_question(lecture, question_text):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–´–• –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–ß—Ç–æ —Ç–∞–∫–æ–µ X?" –∏–ª–∏ "X - —ç—Ç–æ..."
    """
    question_normalized = normalize_text(question_text)
    
    for phrase in ['–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ', '—ç—Ç–æ –æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å']:
        question_normalized = question_normalized.replace(phrase, '')
    
    question_normalized = re.sub(r'\s+\w+\s*\.?\s*$', '', question_normalized)
    question_normalized = question_normalized.strip()
    
    stop_words = {'—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–æ–±–æ–π', 
                  '–∏–ª–∏', '–¥–ª—è', '–ø—Ä–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–Ω–∏—Ö', '–æ–Ω–∞', '–æ–Ω–æ', '–∫–æ—Ç–æ—Ä—ã–µ'}
    question_keywords = [w for w in question_normalized.split() 
                        if len(w) > 3 and w not in stop_words]
    
    if len(question_keywords) < 2:
        return None
    
    patterns = [
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+—è–≤–ª—è–µ—Ç—Å—è\s+([^.!?]{15,500}[.!?])',
    ]
    
    best_match = None
    best_score = 0.0
    
    for pattern in patterns:
        for match in re.finditer(pattern, lecture, re.IGNORECASE):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            
            term_parts = term.split()
            cleaned_parts = []
            for i, part in enumerate(term_parts):
                if i == 0 or part.lower() != term_parts[i-1].lower():
                    cleaned_parts.append(part)
            term = ' '.join(cleaned_parts)
            
            similarity = calculate_similarity(definition, question_text)
            question_ngrams = set(extract_ngrams(question_text, 2, 3))
            definition_ngrams = set(extract_ngrams(definition, 2, 3))
            ngram_overlap = len(question_ngrams.intersection(definition_ngrams))
            ngram_score = ngram_overlap / max(len(question_ngrams), 1)
            combined_score = (similarity * 0.6) + (ngram_score * 0.4)
            
            if combined_score > best_score and combined_score > 0.4:
                best_score = combined_score
                best_match = {
                    "term": term,
                    "definition": definition,
                    "position": match.start(),
                    "score": combined_score
                }
    
    return best_match

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –û–ü–¶–ò–ô ===

def score_option_universal(lecture, option, question, relevant_list=None):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–ø—Ü–∏–∏.
    relevant_list: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ –ª–µ–∫—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å—É
    """
    L = normalize_text(lecture)
    opt = normalize_text(option)
    q = normalize_text(question)
    
    score = 0.0
    snippets = []
    
    # === 0. –ë–û–ù–£–° –ó–ê –í–•–û–ñ–î–ï–ù–ò–ï –í –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –°–ü–ò–°–û–ö ===
    list_bonus = 0.0
    if relevant_list:
        for list_item in relevant_list['items']:
            item_norm = normalize_text(list_item)
            
            if opt in item_norm or item_norm in opt:
                list_bonus = 10.0
                snippets.append({
                    "why": "list_match",
                    "excerpt": f"–ò–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è: {list_item}"
                })
                break
            
            opt_ngrams = set(extract_ngrams(option, 2, 3))
            item_ngrams = set(extract_ngrams(list_item, 2, 3))
            overlap = len(opt_ngrams.intersection(item_ngrams))
            
            if overlap >= 2:
                similarity = calculate_similarity(option, list_item)
                if similarity > 0.6:
                    list_bonus = 7.0
                    snippets.append({
                        "why": f"list_partial (sim: {similarity:.2f})",
                        "excerpt": f"–ò–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è: {list_item}"
                    })
                    break
    
    score += list_bonus
    
    # === 1. –ü–û–ò–°–ö –¶–ï–õ–´–• –°–õ–û–í ===
    word_pattern = r'\b' + re.escape(opt) + r'\b'
    exact_matches = list(re.finditer(word_pattern, L))
    exact_count = len(exact_matches)
    
    if exact_count > 0:
        base_score = 2.0 * (1 + exact_count)**0.3
        best_snippet = None
        best_context_score = 0.0
        
        for match in exact_matches:
            match_pos = match.start()
            context_start = max(0, match_pos - 350)
            context_end = min(len(L), match_pos + 350)
            context = L[context_start:context_end]
            
            question_ngrams = set(extract_ngrams(question, 2, 4))
            context_ngrams = set(extract_ngrams(context, 2, 4))
            common_ngrams = question_ngrams.intersection(context_ngrams)
            context_similarity = calculate_similarity(question, context)
            context_score = (len(common_ngrams) * 0.5) + (context_similarity * 2.0)
            
            if context_score > best_context_score:
                best_context_score = context_score
                orig_pos = lecture.lower().find(opt, match_pos - 10)
                if orig_pos != -1:
                    best_snippet = extract_full_sentences(lecture, orig_pos, 2)
        
        if best_context_score > 0.5:
            context_multiplier = 1 + (best_context_score * 0.8)
            base_score *= context_multiplier
            if best_snippet:
                snippets.append({
                    "why": f"context (score: {best_context_score:.2f})",
                    "excerpt": best_snippet
                })
        else:
            if best_snippet:
                snippets.append({"why": "exact", "excerpt": best_snippet})
        
        score += base_score
    
    # === 2. –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===
    def_patterns = [
        rf"\b{re.escape(opt)}\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]+[.!?])",
        rf"\b{re.escape(opt)}\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]+[.!?])",
    ]
    
    for pat in def_patterns:
        for match in re.finditer(pat, lecture, re.IGNORECASE):
            definition = match.group(1) if len(match.groups()) > 0 else ""
            def_similarity = calculate_similarity(definition, question)
            bonus = 3.0 * (1 + def_similarity)
            score += bonus
            full_sentence = extract_full_sentences(lecture, match.start(), 2)
            snippets.append({
                "why": f"definition (sim: {def_similarity:.2f})",
                "excerpt": full_sentence
            })
    
    # === 3. TF-IDF –í–°–ï–ô –û–ü–¶–ò–ò ===
    opt_words = set(opt.split())
    if opt_words and len(opt_words) > 1:
        matched_words = len(opt_words.intersection(set(L.split())))
        ratio = matched_words / len(opt_words)
        score += ratio * 1.2
    
    return {"score": score, "snippets": snippets}

# === –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –í–û–ü–†–û–°–ê ===

def detect_question_type(qtext):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞"""
    q = normalize_text(qtext)
    
    if re.search(r'(–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ|—Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|–≤–ø–∏—à–∏—Ç–µ|–≤–≤–µ–¥–∏—Ç–µ)', qtext, re.IGNORECASE):
        return 'short'
    
    if re.search(r'–µ–¥–∏–Ω–∏—Ü.*–∏–∑–º–µ—Ä–µ–Ω–∏—è', q):
        return 'units'
    
    single_markers = ['–∫–∞–∫–æ–µ –∏–∑', '–∫–∞–∫–æ–π –∏–∑', '–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '—á—Ç–æ –∏–∑', '—á—Ç–æ —Ç–∞–∫–æ–µ']
    for marker in single_markers:
        if marker in q:
            return 'single'
    
    multi_markers = ['–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤—Ö–æ–¥—è—Ç', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è', '–Ω–∞–∑–æ–≤–∏—Ç–µ', '–¥–µ–π—Å—Ç–≤–∏—è']
    for marker in multi_markers:
        if marker in q:
            return 'multi'
    
    return 'single'

def parse_html_quiz(html):
    """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Ç–µ—Å—Ç–∞"""
    soup = BeautifulSoup(html, 'html.parser')
    questions = []
    
    que_elements = soup.find_all(class_='que')
    
    for el in que_elements:
        q = {}
        qtext_el = el.find(class_='qtext')
        if qtext_el:
            for tag in qtext_el.find_all(['label', 'input']):
                tag.decompose()
            q['question'] = qtext_el.get_text(strip=True).replace('\n', ' ')
        else:
            q['question'] = f"–í–æ–ø—Ä–æ—Å {len(questions) + 1}"
        
        opts = []
        answer_divs = el.find_all(class_='answer')
        for div in answer_divs:
            labels = div.find_all(attrs={'data-region': 'answer-label'})
            for label in labels:
                opt_text = label.get_text(strip=True).replace('\n', ' ')
                if opt_text:
                    opts.append(opt_text)
            for label in div.find_all('label'):
                if not label.find_parent(class_='qtext'):
                    opt_text = label.get_text(strip=True).replace('\n', ' ')
                    if opt_text and opt_text not in opts:
                        opts.append(opt_text)
        
        q['options'] = list(set(opts))
        q['is_short'] = bool(el.find('input', type='text')) or 'shortanswer' in el.get('class', [])
        questions.append(q)
    
    return questions

# === API ENDPOINTS ===

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    if IS_LOCAL:
        mode = "üöÄ –õ–æ–∫–∞–ª—å–Ω—ã–π (SBERT+SpaCy)"
    else:
        mode = "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π (SBERT Lite)" if SBERT_MODEL else "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π (TF-IDF)"
    return templates.TemplateResponse("index.html", {"request": request, "mode": mode})

@app.post("/api/extract-text-from-pdf/")
async def extract_text_from_pdf(file: UploadFile = File(...)):
    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å 10MB)")
    
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")
    
    SESSION_STORAGE["default"] = text
    return {"text": text, "length": len(text), "snippet": text[:200]}

@app.post("/api/parse-quiz-html/")
async def parse_quiz_html(data: QuizHtmlRequest):
    try:
        questions = parse_html_quiz(data.html)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return {"ok": True, "questions": questions}

@app.post("/api/process-quiz/")
async def process_quiz(data: ProcessQuizRequest):
    questions = data.questions
    lecture_text = data.lecture_text
    
    if not lecture_text or not questions:
        raise HTTPException(status_code=400, detail="–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ")
    
    lecture_lists = extract_lists_from_lecture(lecture_text)
    print(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å–ø–∏—Å–∫–æ–≤ –∏–∑ –ª–µ–∫—Ü–∏–∏: {len(lecture_lists)}")
    for i, lst in enumerate(lecture_lists):
        print(f"  –°–ø–∏—Å–æ–∫ {i+1}: {lst['context'][:50]}... ({len(lst['items'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
    
    results = []
    
    for q in questions:
        qtext = q.get("question", "")
        qtype = detect_question_type(qtext)
        opts = q.get("options", [])
        is_short = q.get("is_short", False)
        
        if is_short or qtype == 'short':
            match = find_definition_for_question(lecture_text, qtext)
            
            if match:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": match["term"],
                    "excerpt": extract_full_sentences(lecture_text, match["position"], 2),
                })
            else:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": "",
                    "excerpt": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ",
                })
            continue
        
        relevant_list = None
        if qtype == 'multi':
            relevant_list = find_relevant_list(qtext, lecture_lists)
            if relevant_list:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ (—ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(relevant_list['items'])})")
                print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {relevant_list['context']}")
                print(f"   –≠–ª–µ–º–µ–Ω—Ç—ã: {relevant_list['items']}")
            else:
                print(f"‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ù–ï –Ω–∞–π–¥–µ–Ω –¥–ª—è multi-–≤–æ–ø—Ä–æ—Å–∞")
        
        scored = []
        for opt in opts:
            score_result = score_option_universal(lecture_text, opt, qtext, relevant_list)
            scored.append({
                "option": opt,
                "score": score_result["score"],
                "snippets": score_result["snippets"]
            })
        
        print(f"üîç Scores –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        for s in scored:
            print(f"   {s['option'][:40]}: {s['score']:.2f}")
        
        max_score = max([s["score"] for s in scored], default=1)
        for s in scored:
            s["norm"] = round(s["score"] / max_score, 3) if max_score > 0 else 0
        
        print(f"üìä Scores –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (max={max_score:.2f}):")
        for s in scored:
            print(f"   {s['option'][:40]}: {s['norm']:.3f}")
        
        selected = []
        
        if qtype == 'single' or qtype == 'units':
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                selected = [{
                    "option": sorted_scores[0]["option"],
                    "score": sorted_scores[0]["norm"],
                    "snippets": sorted_scores[0]["snippets"]
                }]
        
        else:  # multi
            sorted_scores = sorted(scored, key=lambda x: x["norm"], reverse=True)
            
            if not sorted_scores:
                selected = []
            else:
                if relevant_list and len(relevant_list['items']) > 0:
                    expected_count = len(relevant_list['items'])
                    print(f"üéØ Multi-—Ä–µ–∂–∏–º —Å–æ —Å–ø–∏—Å–∫–æ–º: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_count} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
                    candidates = [s for s in sorted_scores if s["norm"] >= 0.3]
                    print(f"   –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å norm>=0.3: {len(candidates)}")
                    
                    if len(candidates) >= expected_count:
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates[:expected_count]
                        ]
                        print(f"   ‚úÖ –í—ã–±—Ä–∞–Ω–æ TOP-{expected_count}")
                    else:
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates
                        ]
                        print(f"   ‚ö†Ô∏è –í—ã–±—Ä–∞–Ω–æ {len(candidates)} (–º–µ–Ω—å—à–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ)")
                else:
                    print(f"üéØ Multi-—Ä–µ–∂–∏–º –ë–ï–ó —Å–ø–∏—Å–∫–∞: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥")
                    top_score = sorted_scores[0]["norm"]
                    
                    if top_score < 0.3:
                        selected = [{
                            "option": sorted_scores[0]["option"],
                            "score": sorted_scores[0]["norm"],
                            "snippets": sorted_scores[0]["snippets"]
                        }]
                    else:
                        threshold = max(top_score * 0.6, 0.35)
                        close_values = [s["norm"] for s in sorted_scores if s["norm"] >= 0.35]
                        if len(close_values) >= 2:
                            mean_top = np.mean(close_values[:3])
                            threshold = min(threshold, mean_top * 0.75)
                        
                        candidates = [s for s in scored if s["norm"] >= threshold]
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates
                        ]
                
                selected.sort(key=lambda x: x["score"], reverse=True)
        
        results.append({
            "question": qtext,
            "type": qtype,
            "options": [{"option": s["option"], "norm": s["norm"], "snippets": s["snippets"]} for s in scored],
            "selected": selected
        })
    
    return {"ok": True, "results": results}
, '', cleaned_item).strip()
                        if len(cleaned_item) > 5:  # –ú–∏–Ω–∏–º—É–º 5 —Å–∏–º–≤–æ–ª–æ–≤
                            cleaned_items.append(cleaned_item)
                    
                    if len(cleaned_items) >= 2:
                        lists.append({
                            'context': match.group(0),
                            'items': cleaned_items,
                            'position': match.start()
                        })
                        print(f"üîç –ù–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫: {match.group(0)} ‚Üí {len(cleaned_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                        break
    
    return lists

def find_relevant_list(question, lists):
    """–ù–∞—Ö–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –∏–∑ –ª–µ–∫—Ü–∏–∏, –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å—É."""
    if not lists:
        return None
    
    q_norm = normalize_text(question)
    best_list = None
    best_score = 0.0
    
    for lst in lists:
        context_norm = normalize_text(lst['context'])
        q_ngrams = set(extract_ngrams(question, 2, 3))
        ctx_ngrams = set(extract_ngrams(lst['context'], 2, 3))
        overlap = len(q_ngrams.intersection(ctx_ngrams))
        similarity = calculate_similarity(question, lst['context'])
        score = overlap * 2 + similarity * 5
        
        if score > best_score:
            best_score = score
            best_list = lst
    
    return best_list if best_score > 2.0 else None

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===

def find_definition_for_question(lecture, question_text):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–∞ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –õ–Æ–ë–´–• –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–ß—Ç–æ —Ç–∞–∫–æ–µ X?" –∏–ª–∏ "X - —ç—Ç–æ..."
    """
    question_normalized = normalize_text(question_text)
    
    for phrase in ['–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ', '—ç—Ç–æ –æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å']:
        question_normalized = question_normalized.replace(phrase, '')
    
    question_normalized = re.sub(r'\s+\w+\s*\.?\s*$', '', question_normalized)
    question_normalized = question_normalized.strip()
    
    stop_words = {'—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–æ–±–æ–π', 
                  '–∏–ª–∏', '–¥–ª—è', '–ø—Ä–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–Ω–∏—Ö', '–æ–Ω–∞', '–æ–Ω–æ', '–∫–æ—Ç–æ—Ä—ã–µ'}
    question_keywords = [w for w in question_normalized.split() 
                        if len(w) > 3 and w not in stop_words]
    
    if len(question_keywords) < 2:
        return None
    
    patterns = [
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]{15,500}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]{2,70})\s+—è–≤–ª—è–µ—Ç—Å—è\s+([^.!?]{15,500}[.!?])',
    ]
    
    best_match = None
    best_score = 0.0
    
    for pattern in patterns:
        for match in re.finditer(pattern, lecture, re.IGNORECASE):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            
            term_parts = term.split()
            cleaned_parts = []
            for i, part in enumerate(term_parts):
                if i == 0 or part.lower() != term_parts[i-1].lower():
                    cleaned_parts.append(part)
            term = ' '.join(cleaned_parts)
            
            similarity = calculate_similarity(definition, question_text)
            question_ngrams = set(extract_ngrams(question_text, 2, 3))
            definition_ngrams = set(extract_ngrams(definition, 2, 3))
            ngram_overlap = len(question_ngrams.intersection(definition_ngrams))
            ngram_score = ngram_overlap / max(len(question_ngrams), 1)
            combined_score = (similarity * 0.6) + (ngram_score * 0.4)
            
            if combined_score > best_score and combined_score > 0.4:
                best_score = combined_score
                best_match = {
                    "term": term,
                    "definition": definition,
                    "position": match.start(),
                    "score": combined_score
                }
    
    return best_match

# === –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –û–ü–¶–ò–ô ===

def score_option_universal(lecture, option, question, relevant_list=None):
    """
    –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–ø—Ü–∏–∏.
    relevant_list: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ –ª–µ–∫—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å—É
    """
    L = normalize_text(lecture)
    opt = normalize_text(option)
    q = normalize_text(question)
    
    score = 0.0
    snippets = []
    
    # === 0. –ë–û–ù–£–° –ó–ê –í–•–û–ñ–î–ï–ù–ò–ï –í –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –°–ü–ò–°–û–ö ===
    list_bonus = 0.0
    if relevant_list:
        for list_item in relevant_list['items']:
            item_norm = normalize_text(list_item)
            
            if opt in item_norm or item_norm in opt:
                list_bonus = 10.0
                snippets.append({
                    "why": "list_match",
                    "excerpt": f"–ò–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è: {list_item}"
                })
                break
            
            opt_ngrams = set(extract_ngrams(option, 2, 3))
            item_ngrams = set(extract_ngrams(list_item, 2, 3))
            overlap = len(opt_ngrams.intersection(item_ngrams))
            
            if overlap >= 2:
                similarity = calculate_similarity(option, list_item)
                if similarity > 0.6:
                    list_bonus = 7.0
                    snippets.append({
                        "why": f"list_partial (sim: {similarity:.2f})",
                        "excerpt": f"–ò–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è: {list_item}"
                    })
                    break
    
    score += list_bonus
    
    # === 1. –ü–û–ò–°–ö –¶–ï–õ–´–• –°–õ–û–í ===
    word_pattern = r'\b' + re.escape(opt) + r'\b'
    exact_matches = list(re.finditer(word_pattern, L))
    exact_count = len(exact_matches)
    
    if exact_count > 0:
        base_score = 2.0 * (1 + exact_count)**0.3
        best_snippet = None
        best_context_score = 0.0
        
        for match in exact_matches:
            match_pos = match.start()
            context_start = max(0, match_pos - 350)
            context_end = min(len(L), match_pos + 350)
            context = L[context_start:context_end]
            
            question_ngrams = set(extract_ngrams(question, 2, 4))
            context_ngrams = set(extract_ngrams(context, 2, 4))
            common_ngrams = question_ngrams.intersection(context_ngrams)
            context_similarity = calculate_similarity(question, context)
            context_score = (len(common_ngrams) * 0.5) + (context_similarity * 2.0)
            
            if context_score > best_context_score:
                best_context_score = context_score
                orig_pos = lecture.lower().find(opt, match_pos - 10)
                if orig_pos != -1:
                    best_snippet = extract_full_sentences(lecture, orig_pos, 2)
        
        if best_context_score > 0.5:
            context_multiplier = 1 + (best_context_score * 0.8)
            base_score *= context_multiplier
            if best_snippet:
                snippets.append({
                    "why": f"context (score: {best_context_score:.2f})",
                    "excerpt": best_snippet
                })
        else:
            if best_snippet:
                snippets.append({"why": "exact", "excerpt": best_snippet})
        
        score += base_score
    
    # === 2. –ü–û–ò–°–ö –û–ü–†–ï–î–ï–õ–ï–ù–ò–ô ===
    def_patterns = [
        rf"\b{re.escape(opt)}\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]+[.!?])",
        rf"\b{re.escape(opt)}\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]+[.!?])",
    ]
    
    for pat in def_patterns:
        for match in re.finditer(pat, lecture, re.IGNORECASE):
            definition = match.group(1) if len(match.groups()) > 0 else ""
            def_similarity = calculate_similarity(definition, question)
            bonus = 3.0 * (1 + def_similarity)
            score += bonus
            full_sentence = extract_full_sentences(lecture, match.start(), 2)
            snippets.append({
                "why": f"definition (sim: {def_similarity:.2f})",
                "excerpt": full_sentence
            })
    
    # === 3. TF-IDF –í–°–ï–ô –û–ü–¶–ò–ò ===
    opt_words = set(opt.split())
    if opt_words and len(opt_words) > 1:
        matched_words = len(opt_words.intersection(set(L.split())))
        ratio = matched_words / len(opt_words)
        score += ratio * 1.2
    
    return {"score": score, "snippets": snippets}

# === –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê –í–û–ü–†–û–°–ê ===

def detect_question_type(qtext):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞"""
    q = normalize_text(qtext)
    
    if re.search(r'(–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ|—Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|–≤–ø–∏—à–∏—Ç–µ|–≤–≤–µ–¥–∏—Ç–µ)', qtext, re.IGNORECASE):
        return 'short'
    
    if re.search(r'–µ–¥–∏–Ω–∏—Ü.*–∏–∑–º–µ—Ä–µ–Ω–∏—è', q):
        return 'units'
    
    single_markers = ['–∫–∞–∫–æ–µ –∏–∑', '–∫–∞–∫–æ–π –∏–∑', '–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '—á—Ç–æ –∏–∑', '—á—Ç–æ —Ç–∞–∫–æ–µ']
    for marker in single_markers:
        if marker in q:
            return 'single'
    
    multi_markers = ['–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤—Ö–æ–¥—è—Ç', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è', '–Ω–∞–∑–æ–≤–∏—Ç–µ', '–¥–µ–π—Å—Ç–≤–∏—è']
    for marker in multi_markers:
        if marker in q:
            return 'multi'
    
    return 'single'

def parse_html_quiz(html):
    """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Ç–µ—Å—Ç–∞"""
    soup = BeautifulSoup(html, 'html.parser')
    questions = []
    
    que_elements = soup.find_all(class_='que')
    
    for el in que_elements:
        q = {}
        qtext_el = el.find(class_='qtext')
        if qtext_el:
            for tag in qtext_el.find_all(['label', 'input']):
                tag.decompose()
            q['question'] = qtext_el.get_text(strip=True).replace('\n', ' ')
        else:
            q['question'] = f"–í–æ–ø—Ä–æ—Å {len(questions) + 1}"
        
        opts = []
        answer_divs = el.find_all(class_='answer')
        for div in answer_divs:
            labels = div.find_all(attrs={'data-region': 'answer-label'})
            for label in labels:
                opt_text = label.get_text(strip=True).replace('\n', ' ')
                if opt_text:
                    opts.append(opt_text)
            for label in div.find_all('label'):
                if not label.find_parent(class_='qtext'):
                    opt_text = label.get_text(strip=True).replace('\n', ' ')
                    if opt_text and opt_text not in opts:
                        opts.append(opt_text)
        
        q['options'] = list(set(opts))
        q['is_short'] = bool(el.find('input', type='text')) or 'shortanswer' in el.get('class', [])
        questions.append(q)
    
    return questions

# === API ENDPOINTS ===

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    if IS_LOCAL:
        mode = "üöÄ –õ–æ–∫–∞–ª—å–Ω—ã–π (SBERT+SpaCy)"
    else:
        mode = "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π (SBERT Lite)" if SBERT_MODEL else "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π (TF-IDF)"
    return templates.TemplateResponse("index.html", {"request": request, "mode": mode})

@app.post("/api/extract-text-from-pdf/")
async def extract_text_from_pdf(file: UploadFile = File(...)):
    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å 10MB)")
    
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")
    
    SESSION_STORAGE["default"] = text
    return {"text": text, "length": len(text), "snippet": text[:200]}

@app.post("/api/parse-quiz-html/")
async def parse_quiz_html(data: QuizHtmlRequest):
    try:
        questions = parse_html_quiz(data.html)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–í–æ–ø—Ä–æ—Å—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return {"ok": True, "questions": questions}

@app.post("/api/process-quiz/")
async def process_quiz(data: ProcessQuizRequest):
    questions = data.questions
    lecture_text = data.lecture_text
    
    if not lecture_text or not questions:
        raise HTTPException(status_code=400, detail="–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ")
    
    lecture_lists = extract_lists_from_lecture(lecture_text)
    print(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å–ø–∏—Å–∫–æ–≤ –∏–∑ –ª–µ–∫—Ü–∏–∏: {len(lecture_lists)}")
    for i, lst in enumerate(lecture_lists):
        print(f"  –°–ø–∏—Å–æ–∫ {i+1}: {lst['context'][:50]}... ({len(lst['items'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
    
    results = []
    
    for q in questions:
        qtext = q.get("question", "")
        qtype = detect_question_type(qtext)
        opts = q.get("options", [])
        is_short = q.get("is_short", False)
        
        if is_short or qtype == 'short':
            match = find_definition_for_question(lecture_text, qtext)
            
            if match:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": match["term"],
                    "excerpt": extract_full_sentences(lecture_text, match["position"], 2),
                })
            else:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": "",
                    "excerpt": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ",
                })
            continue
        
        relevant_list = None
        if qtype == 'multi':
            relevant_list = find_relevant_list(qtext, lecture_lists)
            if relevant_list:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ (—ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(relevant_list['items'])})")
                print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {relevant_list['context']}")
                print(f"   –≠–ª–µ–º–µ–Ω—Ç—ã: {relevant_list['items']}")
            else:
                print(f"‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ù–ï –Ω–∞–π–¥–µ–Ω –¥–ª—è multi-–≤–æ–ø—Ä–æ—Å–∞")
        
        scored = []
        for opt in opts:
            score_result = score_option_universal(lecture_text, opt, qtext, relevant_list)
            scored.append({
                "option": opt,
                "score": score_result["score"],
                "snippets": score_result["snippets"]
            })
        
        print(f"üîç Scores –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        for s in scored:
            print(f"   {s['option'][:40]}: {s['score']:.2f}")
        
        max_score = max([s["score"] for s in scored], default=1)
        for s in scored:
            s["norm"] = round(s["score"] / max_score, 3) if max_score > 0 else 0
        
        print(f"üìä Scores –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (max={max_score:.2f}):")
        for s in scored:
            print(f"   {s['option'][:40]}: {s['norm']:.3f}")
        
        selected = []
        
        if qtype == 'single' or qtype == 'units':
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                selected = [{
                    "option": sorted_scores[0]["option"],
                    "score": sorted_scores[0]["norm"],
                    "snippets": sorted_scores[0]["snippets"]
                }]
        
        else:  # multi
            sorted_scores = sorted(scored, key=lambda x: x["norm"], reverse=True)
            
            if not sorted_scores:
                selected = []
            else:
                if relevant_list and len(relevant_list['items']) > 0:
                    expected_count = len(relevant_list['items'])
                    print(f"üéØ Multi-—Ä–µ–∂–∏–º —Å–æ —Å–ø–∏—Å–∫–æ–º: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_count} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
                    candidates = [s for s in sorted_scores if s["norm"] >= 0.3]
                    print(f"   –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å norm>=0.3: {len(candidates)}")
                    
                    if len(candidates) >= expected_count:
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates[:expected_count]
                        ]
                        print(f"   ‚úÖ –í—ã–±—Ä–∞–Ω–æ TOP-{expected_count}")
                    else:
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates
                        ]
                        print(f"   ‚ö†Ô∏è –í—ã–±—Ä–∞–Ω–æ {len(candidates)} (–º–µ–Ω—å—à–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ)")
                else:
                    print(f"üéØ Multi-—Ä–µ–∂–∏–º –ë–ï–ó —Å–ø–∏—Å–∫–∞: –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥")
                    top_score = sorted_scores[0]["norm"]
                    
                    if top_score < 0.3:
                        selected = [{
                            "option": sorted_scores[0]["option"],
                            "score": sorted_scores[0]["norm"],
                            "snippets": sorted_scores[0]["snippets"]
                        }]
                    else:
                        threshold = max(top_score * 0.6, 0.35)
                        close_values = [s["norm"] for s in sorted_scores if s["norm"] >= 0.35]
                        if len(close_values) >= 2:
                            mean_top = np.mean(close_values[:3])
                            threshold = min(threshold, mean_top * 0.75)
                        
                        candidates = [s for s in scored if s["norm"] >= threshold]
                        selected = [
                            {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                            for s in candidates
                        ]
                
                selected.sort(key=lambda x: x["score"], reverse=True)
        
        results.append({
            "question": qtext,
            "type": qtype,
            "options": [{"option": s["option"], "norm": s["norm"], "snippets": s["snippets"]} for s in scored],
            "selected": selected
        })
    
    return {"ok": True, "results": results}