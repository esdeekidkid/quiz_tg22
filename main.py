import io
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import pdfplumber
from bs4 import BeautifulSoup
import re
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
IS_LOCAL = os.getenv('ENVIRONMENT') != 'production'

# –ó–∞–≥—Ä—É–∂–∞–µ–º SBERT —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ
if IS_LOCAL:
    try:
        from sentence_transformers import SentenceTransformer, util
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é SBERT –º–æ–¥–µ–ª—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞...")
        SBERT_MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        print("‚úÖ SBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    except ImportError:
        print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF")
        SBERT_MODEL = None
else:
    SBERT_MODEL = None
    print("‚òÅÔ∏è Render —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—ë–≥–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã")

app = FastAPI(title="Quiz Helper API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

SESSION_STORAGE = {}

class QuizHtmlRequest(BaseModel):
    html: str = Field(..., min_length=1)

class ProcessQuizRequest(BaseModel):
    questions: List[Dict[str, Any]]
    lecture_text: str = Field(..., min_length=1)

# === –£–¢–ò–õ–ò–¢–´ ===

def normalize_text(s):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not s:
        return ""
    return re.sub(r'\s+', ' ', s).strip().lower()

def extract_full_sentences(text, position, num_sentences=2):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text or position < 0:
        return ""
    
    sentence_start = position
    for i in range(position - 1, -1, -1):
        if text[i] in '.!?\n' and i > 0:
            sentence_start = i + 1
            break
        elif i == 0:
            sentence_start = 0
    
    sentence_end = position
    sentences_found = 0
    for i in range(position, len(text)):
        if text[i] in '.!?':
            sentences_found += 1
            if sentences_found >= num_sentences:
                sentence_end = i + 1
                break
    
    if sentences_found < num_sentences:
        sentence_end = min(len(text), position + 500)
    
    result = text[sentence_start:sentence_end].strip()
    return result

def calculate_text_similarity_tfidf(text1, text2):
    """TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–ª—ë–≥–∫–∏–π –º–µ—Ç–æ–¥)"""
    try:
        vectorizer = TfidfVectorizer(min_df=1, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)
    except:
        # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        words1 = set(normalize_text(text1).split())
        words2 = set(normalize_text(text2).split())
        if not words1 or not words2:
            return 0.0
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0

def calculate_text_similarity_sbert(text1, text2):
    """SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (—Ç—è–∂—ë–ª—ã–π –º–µ—Ç–æ–¥)"""
    if SBERT_MODEL is None:
        return calculate_text_similarity_tfidf(text1, text2)
    
    try:
        embeddings = SBERT_MODEL.encode([text1, text2], convert_to_tensor=True)
        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()
        return float(similarity)
    except:
        return calculate_text_similarity_tfidf(text1, text2)

def calculate_text_similarity(text1, text2):
    """–í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    if IS_LOCAL and SBERT_MODEL is not None:
        return calculate_text_similarity_sbert(text1, text2)
    else:
        return calculate_text_similarity_tfidf(text1, text2)

def extract_ngrams(text, n=3):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç n-–≥—Ä–∞–º–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    words = normalize_text(text).split()
    if len(words) < n:
        return [' '.join(words)]
    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]

def calculate_ngram_overlap(text1, text2, n=3):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ n-–≥—Ä–∞–º–º"""
    ngrams1 = set(extract_ngrams(text1, n))
    ngrams2 = set(extract_ngrams(text2, n))
    
    if not ngrams1:
        return 0.0
    
    overlap = len(ngrams1.intersection(ngrams2))
    return overlap / len(ngrams1)

def find_definition_for_question(lecture, question_text):
    """–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ—Ä–º–∏–Ω –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
    question_normalized = normalize_text(question_text)
    
    for phrase in ['–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ', '—ç—Ç–æ –æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ']:
        question_normalized = question_normalized.replace(phrase, '')
    
    question_normalized = question_normalized.strip()
    
    stop_words = {'—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–æ–±–æ–π', 
                  '–∏–ª–∏', '–¥–ª—è', '–ø—Ä–∏', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–Ω–∏—Ö', '–æ–Ω–∞', '–æ–Ω–æ'}
    question_keywords = [w for w in question_normalized.split() 
                        if len(w) > 3 and w not in stop_words]
    
    if len(question_keywords) < 3:
        return None
    
    patterns = [
        r'([–ê-–Ø–Å][–∞-—è—ë\s\-]{2,60})\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]{20,400}[.!?])',
        r'([–ê-–Ø–Å][–∞-—è—ë\s\-]{2,60})\s+[‚Äî\-]\s+([^.!?]{20,400}[.!?])',
    ]
    
    best_match = None
    best_score = 0.0
    
    for pattern in patterns:
        for match in re.finditer(pattern, lecture, re.IGNORECASE):
            term = match.group(1).strip()
            definition = match.group(2).strip()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = calculate_text_similarity(definition, question_text)
            
            # N-gram overlap –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            ngram_score = calculate_ngram_overlap(definition, question_text, 3)
            
            # –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            definition_normalized = normalize_text(definition)
            keyword_matches = sum(1 for kw in question_keywords if kw in definition_normalized)
            keyword_ratio = keyword_matches / len(question_keywords) if question_keywords else 0
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä (–±–æ–ª—å—à–∏–π –≤–µ—Å –Ω–∞ similarity)
            combined_score = (similarity * 0.5) + (keyword_ratio * 0.3) + (ngram_score * 0.2)
            
            if combined_score > best_score and combined_score > 0.5:
                best_score = combined_score
                best_match = {
                    "term": term,
                    "definition": definition,
                    "position": match.start(),
                    "score": combined_score
                }
    
    return best_match

def extract_key_concepts_from_question(question):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
    q_lower = normalize_text(question)
    concepts = []
    
    if '–µ–¥–∏–Ω–∏—Ü' in q_lower and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in q_lower:
        if '—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω' in q_lower:
            concepts.append('—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω')
        if '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω' in q_lower:
            concepts.append('—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω')
        if '–¥–æ–∑' in q_lower:
            concepts.append('–¥–æ–∑')
    
    if '–∏–∑–ª—É—á' in q_lower:
        if '—ç–ª–µ–∫—Ç—Ä–æ–º–∞–≥–Ω–∏—Ç–Ω' in q_lower:
            concepts.append('—ç–ª–µ–∫—Ç—Ä–æ–º–∞–≥–Ω–∏—Ç–Ω')
        if '–∫–æ—Ä–ø—É—Å–∫—É–ª—è—Ä–Ω' in q_lower:
            concepts.append('–∫–æ—Ä–ø—É—Å–∫—É–ª—è—Ä–Ω')
        if '–ø—Ä–æ–Ω–∏–∫–∞—é—â' in q_lower:
            concepts.append('–ø—Ä–æ–Ω–∏–∫–∞—é—â')
        if '–∏–æ–Ω–∏–∑–∏—Ä—É—é—â' in q_lower:
            concepts.append('–∏–æ–Ω–∏–∑–∏—Ä—É—é—â')
        if '–º–∞–ª—ã–º –∏–æ–Ω–∏–∑–∏—Ä—É—é—â–∏–º' in q_lower or '–º–∞–ª–æ–µ –∏–æ–Ω–∏–∑–∏—Ä—É—é—â–µ–µ' in q_lower:
            concepts.append('–º–∞–ª—ã–º_–∏–æ–Ω–∏–∑–∏—Ä—É—é—â–∏–º')
        if '–±–æ–ª—å—à–æ–π –ø—Ä–æ–Ω–∏–∫–∞—é—â–µ–π' in q_lower or '–±–æ–ª—å—à–∞—è –ø—Ä–æ–Ω–∏–∫–∞—é—â–∞—è' in q_lower:
            concepts.append('–±–æ–ª—å—à–æ–π_–ø—Ä–æ–Ω–∏–∫–∞—é—â–µ–π')
    
    return concepts

def score_option_by_lecture(lecture, option, question=""):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–ø—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–µ–∫—Ü–∏–∏ —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–ø—Ä–æ—Å–∞"""
    L = normalize_text(lecture)
    opt = normalize_text(option)
    q = normalize_text(question)
    
    score = 0
    snippets = []
    
    key_concepts = extract_key_concepts_from_question(question)
    
    exact_pattern = re.escape(opt)
    exact_matches = list(re.finditer(exact_pattern, L))
    exact_count = len(exact_matches)
    
    if exact_count > 0:
        base_score = 2.5 * (1 + exact_count)**0.4
        
        best_context_score = 0
        best_context_snippet = None
        
        for match in exact_matches:
            match_pos = match.start()
            
            context_start = max(0, match_pos - 300)
            context_end = min(len(L), match_pos + 300)
            context = L[context_start:context_end]
            
            context_score = sum(1 for concept in key_concepts if concept in context)
            
            if context_score > best_context_score:
                best_context_score = context_score
                orig_pos = lecture.lower().find(opt, match_pos - 10)
                if orig_pos != -1:
                    best_context_snippet = extract_full_sentences(lecture, orig_pos, 2)
        
        if best_context_score > 0:
            base_score *= (1 + best_context_score * 0.8)
            if best_context_snippet:
                snippets.append({
                    "why": f"exact_context (concepts: {best_context_score})",
                    "excerpt": best_context_snippet
                })
        else:
            if key_concepts:
                base_score *= 0.2
            if best_context_snippet:
                snippets.append({
                    "why": "exact",
                    "excerpt": best_context_snippet
                })
        
        score += base_score
    
    def_patterns = [
        rf"{re.escape(opt)}\s*[‚Äî\-:]\s*—ç—Ç–æ\s+([^.!?]+[.!?])",
        rf"{re.escape(opt)}\s+[‚Äî\-]\s+([^.!?]+[.!?])",
        rf"{re.escape(opt)}\s+–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç\s+—Å–æ–±–æ–π\s+([^.!?]+[.!?])",
    ]
    
    for pat in def_patterns:
        for match in re.finditer(pat, lecture, re.IGNORECASE):
            match_text = match.group(0)
            definition = match.group(1) if len(match.groups()) > 0 else ""
            
            def_normalized = normalize_text(definition)
            q_words = [w for w in q.split() if len(w) > 3]
            match_count = sum(1 for w in q_words if w in def_normalized)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
            concept_in_def = sum(1 for concept in key_concepts if concept in def_normalized)
            
            bonus = 4.0
            if match_count > 0:
                bonus *= (1 + match_count * 0.3)
            if concept_in_def > 0:
                bonus *= (1 + concept_in_def * 0.4)
            
            score += bonus
            full_sentence = extract_full_sentences(lecture, match.start(), 2)
            snippets.append({
                "why": f"definition (q_words: {match_count}, concepts: {concept_in_def})",
                "excerpt": full_sentence
            })
    
    opt_words = set(opt.split())
    if opt_words and len(opt_words) > 1:
        matched_words = len(opt_words.intersection(set(L.split())))
        ratio = matched_words / len(opt_words)
        score += ratio * 1.5
        if ratio > 0:
            snippets.append({
                "why": "word-match",
                "matched": f"{matched_words}/{len(opt_words)}"
            })
    
    return {"score": score, "snippets": snippets}

def detect_question_type(qtext):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞"""
    q = normalize_text(qtext)
    
    if re.search(r'(–∫–∞–∫–æ–µ —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|—Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ|–≤–ø–∏—à–∏—Ç–µ|–≤–≤–µ–¥–∏—Ç–µ)', qtext, re.IGNORECASE):
        return 'short'
    
    if '–µ–¥–∏–Ω–∏—Ü' in q and '–∏–∑–º–µ—Ä–µ–Ω–∏—è' in q:
        return 'units'
    
    single_markers = ['–∫–∞–∫–æ–µ –∏–∑', '–∫–∞–∫–æ–π –∏–∑', '–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è', '—á—Ç–æ –∏–∑', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫–æ–µ .* –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç']
    for marker in single_markers:
        if re.search(marker, q):
            return 'single'
    
    multi_markers = ['–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–≤—Ö–æ–¥—è—Ç –≤', '–æ—Ç–Ω–æ—Å—è—Ç—Å—è', '–Ω–∞–∑–æ–≤–∏—Ç–µ –≤—Å–µ', '–∫–∞–∫–æ–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è']
    for marker in multi_markers:
        if marker in q:
            return 'multi'
    
    return 'single'

def parse_html_quiz(html):
    """–ü–∞—Ä—Å–∏—Ç HTML —Ç–µ—Å—Ç–∞"""
    soup = BeautifulSoup(html, 'html.parser')
    questions = []
    
    que_elements = soup.find_all(class_='que')
    
    for el in que_elements:
        q = {}
        qtext_el = el.find(class_='qtext')
        if qtext_el:
            for tag in qtext_el.find_all(['label', 'input']):
                tag.decompose()
            q['question'] = qtext_el.get_text(strip=True).replace('\n', ' ')
        else:
            q['question'] = f"–í–æ–ø—Ä–æ—Å {len(questions) + 1}"
        
        opts = []
        answer_divs = el.find_all(class_='answer')
        for div in answer_divs:
            labels = div.find_all(attrs={'data-region': 'answer-label'})
            for label in labels:
                opt_text = label.get_text(strip=True).replace('\n', ' ')
                if opt_text:
                    opts.append(opt_text)
            for label in div.find_all('label'):
                if not label.find_parent(class_='qtext'):
                    opt_text = label.get_text(strip=True).replace('\n', ' ')
                    if opt_text and opt_text not in opts:
                        opts.append(opt_text)
        
        q['options'] = list(set(opts))
        q['is_short'] = bool(el.find('input', type='text')) or 'shortanswer' in el.get('class', [])
        questions.append(q)
    
    if not questions:
        all_ps = soup.find_all('p')
        all_inputs = soup.find_all('input', type='radio') + soup.find_all('input', type='checkbox')
        for p in all_ps:
            if p.find('input'):
                continue
            question_text = p.get_text(strip=True).replace('\n', ' ')
            if question_text:
                options = []
                for inp in all_inputs:
                    label = soup.find('label', attrs={'for': inp.get('id')})
                    if label:
                        opt_text = label.get_text(strip=True).replace('\n', ' ')
                        if opt_text:
                            options.append(opt_text)
                questions.append({
                    "question": question_text,
                    "options": list(set(options)),
                    "is_short": bool(soup.find('input', type='text'))
                })
                break
    
    return questions

# === –ú–ê–†–®–†–£–¢–´ ===

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    mode = "üöÄ –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (SBERT)" if (IS_LOCAL and SBERT_MODEL) else "‚òÅÔ∏è –û–±–ª–∞—á–Ω—ã–π —Ä–µ–∂–∏–º (TF-IDF)"
    return templates.TemplateResponse("index.html", {"request": request, "mode": mode})

@app.post("/api/extract-text-from-pdf/")
async def extract_text_from_pdf(file: UploadFile = File(...)):
    if file.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF")
    
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 10MB)")
    
    try:
        with pdfplumber.open(io.BytesIO(content)) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
    
    session_key = "default"
    SESSION_STORAGE[session_key] = text
    
    return {
        "text": text,
        "length": len(text),
        "snippet": text[:200]
    }

@app.post("/api/parse-quiz-html/")
async def parse_quiz_html(data: QuizHtmlRequest):
    html = data.html
    if not html:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'html' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    try:
        questions = parse_html_quiz(html)
    except Exception as e:
        print(f"Error in parse_html_quiz: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML —Ç–µ—Å—Ç–∞: {str(e)}")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–í HTML –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.")
    
    return {"ok": True, "questions": questions}

@app.post("/api/process-quiz/")
async def process_quiz(data: ProcessQuizRequest):
    questions = data.questions
    lecture_text = data.lecture_text
    
    if not lecture_text:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'lecture_text' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    if not questions:
        raise HTTPException(status_code=400, detail="–ü–æ–ª–µ 'questions' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–æ.")
    
    results = []
    
    for q in questions:
        qtext = q.get("question", "")
        qtype = detect_question_type(qtext)
        opts = q.get("options", [])
        is_short = q.get("is_short", False)
        
        if is_short or qtype == 'short':
            match = find_definition_for_question(lecture_text, qtext)
            
            if match:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": match["term"],
                    "excerpt": extract_full_sentences(lecture_text, match["position"], 2),
                })
            else:
                results.append({
                    "question": qtext,
                    "type": "short",
                    "answer": "",
                    "excerpt": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ",
                })
            continue
        
        scored = []
        for opt in opts:
            score_result = score_option_by_lecture(lecture_text, opt, qtext)
            scored.append({
                "option": opt,
                "score": score_result["score"],
                "snippets": score_result["snippets"]
            })
        
        max_score = max([s["score"] for s in scored], default=1)
        for s in scored:
            s["norm"] = round(s["score"] / max_score, 3) if max_score > 0 else 0
        
        selected = []
        
        if qtype == 'single':
            sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
            if sorted_scores:
                top = sorted_scores[0]
                selected = [{
                    "option": top["option"],
                    "score": top["norm"],
                    "snippets": top["snippets"]
                }]
        
        elif qtype == 'units':
            key_concepts = extract_key_concepts_from_question(qtext)
            
            if key_concepts:
                context_matches = []
                for s in scored:
                    has_strong_context = any(
                        'exact_context' in snippet.get('why', '') and 'concepts:' in snippet.get('why', '')
                        for snippet in s['snippets']
                    )
                    if has_strong_context:
                        context_matches.append(s)
                
                context_matches.sort(key=lambda x: x["score"], reverse=True)
                
                if context_matches:
                    max_context_score = context_matches[0]["score"]
                    selected = [
                        {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                        for s in context_matches
                        if s["score"] >= max_context_score * 0.8
                    ]
                else:
                    sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
                    if sorted_scores:
                        selected = [{
                            "option": sorted_scores[0]["option"],
                            "score": sorted_scores[0]["norm"],
                            "snippets": sorted_scores[0]["snippets"]
                        }]
            else:
                sorted_scores = sorted(scored, key=lambda x: x["score"], reverse=True)
                if sorted_scores:
                    selected = [{
                        "option": sorted_scores[0]["option"],
                        "score": sorted_scores[0]["norm"],
                        "snippets": sorted_scores[0]["snippets"]
                    }]
        
        else:  # multi
            candidates = [s for s in scored if s["norm"] >= 0.5]
            if not candidates:
                candidates = [s for s in scored if s["norm"] >= 0.3]
            selected = [
                {"option": s["option"], "score": s["norm"], "snippets": s["snippets"]}
                for s in candidates
            ]
            selected.sort(key=lambda x: x["score"], reverse=True)
        
        results.append({
            "question": qtext,
            "type": qtype,
            "options": [{"option": s["option"], "norm": s["norm"], "snippets": s["snippets"]} for s in scored],
            "selected": selected
        })
    
    return {"ok": True, "results": results}
